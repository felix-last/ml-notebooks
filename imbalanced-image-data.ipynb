{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook contains the experimental procedure in order to compare and visualize various undersampling and oversampling methods on simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.1 General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.2 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.datasets import make_classification, fetch_mldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.3 imbalance-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.datasets import make_imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3 Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.1 Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def measures(estimator, X, y):\n",
    "    y_predicted = estimator.predict(X)\n",
    "    true_positive = (y_predicted == 1)[y == 1].sum()\n",
    "    false_positive = (y_predicted == 1)[y == 0].sum()\n",
    "    true_negative = (y_predicted == 0)[y == 0].sum()\n",
    "    false_negative = (y_predicted == 0)[y == 1].sum()\n",
    "    return true_positive, false_positive, true_negative, false_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.2 F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def F_measure(estimator, X, y):\n",
    "    true_positive, false_positive, true_negative, false_negative = measures(estimator, X, y)\n",
    "    return 2 * true_positive / (2 * true_positive + false_positive + false_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.3 G-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def G_mean(estimator, X, y):\n",
    "    true_positive, false_positive, true_negative, false_negative = measures(estimator, X, y)\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    return sqrt(sensitivity * specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4 Load image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.1 Datasets parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "mnist = fetch_mldata(\"MNIST\", data_home=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.2 Append datasets to container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "imbalanced_datasets = []\n",
    "for parameters in datasets_parameters:\n",
    "    imbalanced_datasets.append(make_classification(**parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.3 Plot datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imbalanced_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f546c28560b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimbalanced_dataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimbalanced_datasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimbalanced_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mminority_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mminority_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mminority_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imbalanced_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "sns.set()\n",
    "for imbalanced_dataset in imbalanced_datasets:\n",
    "    X, y = imbalanced_dataset\n",
    "    minority_indices = (y == 1)\n",
    "    p = plt.scatter(X[minority_indices, 0], X[minority_indices, 1], color=\"blue\")\n",
    "    p = plt.scatter(X[~minority_indices, 0], X[~minority_indices, 1], color=\"red\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of features</th>\n",
       "      <th># of instances</th>\n",
       "      <th># of minority instances</th>\n",
       "      <th># of majority instances</th>\n",
       "      <th>Imbalanced Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>25</td>\n",
       "      <td>475</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>102</td>\n",
       "      <td>898</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  # of features # of instances  # of minority instances  \\\n",
       "0             2            500                       25   \n",
       "1             2           1000                      102   \n",
       "\n",
       "   # of majority instances  Imbalanced Ratio  \n",
       "0                      475                19  \n",
       "1                      898                 8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_description = pd.DataFrame({}, columns=[\"# of features\", \"# of instances\", \"# of minority instances\", \"# of majority instances\", \"Imbalanced Ratio\"])\n",
    "for ind, (X, y) in enumerate(imbalanced_datasets):\n",
    "    num_features = X.shape[1]\n",
    "    num_instances = y.size\n",
    "    num_minority_instances = (y == 1).sum()\n",
    "    num_majority_instances = (y == 0).sum()\n",
    "    IR = round((y == 0).sum() / (y == 1).sum(), 2)\n",
    "    datasets_description.loc[len(datasets_description)] = [num_features, num_instances, num_minority_instances, num_majority_instances, IR]\n",
    "data_types = [\"object\", \"object\", \"int64\", \"int64\", \"int64\", \"int64\", \"float64\"]\n",
    "for ind, feature in enumerate(datasets_description.columns):\n",
    "    datasets_description[feature] = datasets_description[feature].astype(data_types[ind])\n",
    "datasets_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5 Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 5.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "metrics = ['roc_auc', F_measure, G_mean]\n",
    "random_states = [5 * i for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.2 Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for random_state in random_states:\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=random_state)\n",
    "    algorithms = [LogisticRegression(random_state=random_state)]\n",
    "    methods = [None, RandomOverSampler(random_state=random_state), SMOTE(random_state=random_state, k_neighbors=3), SMOTE(random_state=random_state, kind='borderline1', k_neighbors=3), ADASYN(random_state=random_state, n_neighbors=3)]\n",
    "    for algorithm in algorithms:\n",
    "        for ind, (X, y) in enumerate(imbalanced_datasets):\n",
    "            for method in methods:\n",
    "                for metric in metrics:\n",
    "                    if method is None:\n",
    "                        clf = algorithm\n",
    "                    else:\n",
    "                        clf = make_pipeline(method, algorithm)\n",
    "                    cv_scores.append(cross_val_score(clf, X, y, cv=cv, scoring=metric).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.3 Results parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "algorithms_names = [\"LR\"]\n",
    "datasets_names = [\"two_dim_data1\", \"two_dim_data2\"]\n",
    "metrics_names = [\"AUC\", \"F\", \"G\"]\n",
    "methods_names = [\"None\", \"Random\", \"SMOTE\", \"Borderline SMOTE\", \"ADASYN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.4 Mean results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mean_cv_scores = np.array(cv_scores).reshape(len(random_states), -1).mean(axis=0).reshape(len(algorithms), len(imbalanced_datasets), len(methods), len(metrics))\n",
    "mean_results = pd.DataFrame()\n",
    "for ind1, alg_name in enumerate(algorithms_names):\n",
    "    for ind2, ds_name in enumerate(datasets_names):\n",
    "        partial_results = pd.DataFrame(mean_cv_scores[ind1, ind2, :, :].transpose(), columns=methods_names)\n",
    "        partial_results.insert(0, \"Metric\", metrics_names)\n",
    "        partial_results.insert(0, \"Dataset\", ds_name)\n",
    "        partial_results.insert(0, \"Algorithm\", alg_name)\n",
    "        mean_results = pd.concat([mean_results, partial_results])\n",
    "mean_results = mean_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.5 Standard deviation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "std_cv_scores = np.array(cv_scores).reshape(len(random_states), -1).std(axis=0).reshape(len(algorithms), len(imbalanced_datasets), len(methods), len(metrics))\n",
    "std_results = pd.DataFrame()\n",
    "for ind1, alg_name in enumerate(algorithms_names):\n",
    "    for ind2, ds_name in enumerate(datasets_names):\n",
    "        partial_results = pd.DataFrame(std_cv_scores[ind1, ind2, :, :].transpose(), columns=methods_names)\n",
    "        partial_results.insert(0, \"Metric\", metrics_names)\n",
    "        partial_results.insert(0, \"Dataset\", ds_name)\n",
    "        partial_results.insert(0, \"Algorithm\", alg_name)\n",
    "        std_results = pd.concat([std_results, partial_results])\n",
    "std_results = std_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.6 Oversampling methods ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>None</th>\n",
       "      <th>Random</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>Borderline SMOTE</th>\n",
       "      <th>ADASYN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">LR</th>\n",
       "      <th>AUC</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  None  Random  SMOTE  Borderline SMOTE  ADASYN\n",
       "Algorithm Metric                                               \n",
       "LR        AUC      3.0     3.5    2.5               3.0     3.0\n",
       "          F        1.5     3.5    2.5               2.5     5.0\n",
       "          G        1.5     3.5    2.5               2.5     5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking = mean_results.apply(lambda row: len(row[3:]) - row[3:].argsort().argsort(), axis=1)\n",
    "aggregated_ranking = round(pd.concat([mean_results[[\"Algorithm\", \"Metric\"]], ranking], axis=1).groupby([\"Algorithm\", \"Metric\"]).mean(), 2)\n",
    "aggregated_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.7 Friedman test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Metric</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>F</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>G</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm Metric  p-value\n",
       "0        LR    AUC     0.98\n",
       "1        LR      F     0.23\n",
       "2        LR      G     0.23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friedman_results = pd.DataFrame({}, columns=[\"Algorithm\", \"Metric\", \"p-value\"])\n",
    "for alg_name in algorithms_names:\n",
    "    for metric_name in metrics_names:\n",
    "        partial_ranking = pd.concat([mean_results[[\"Algorithm\", \"Metric\"]], ranking], axis=1)[(mean_results[\"Algorithm\"] == alg_name) & (mean_results[\"Metric\"] == metric_name)]\n",
    "        friedman_inputs = []\n",
    "        for method_name in methods_names:\n",
    "            friedman_inputs.append(partial_ranking[method_name])\n",
    "        pvalue = round(scipy.stats.friedmanchisquare(*friedman_inputs).pvalue, 2)\n",
    "        friedman_results.loc[len(friedman_results)] = [alg_name, metric_name, pvalue]\n",
    "friedman_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
